{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPLab1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOr2D88o5iIgU6kwDTCDkke"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HXsYlE5hscO"
      },
      "outputs": [],
      "source": [
        "document_corpus = [\"this is good phone phone\" , \n",
        "                   \"this is bad mobile mobile\" , \n",
        "                   \"she is good good cat\" , \n",
        "                   \"he has bad temper temper\" , \n",
        "                   \"this mobile phone phone is not good good\"\n",
        "                  ]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "i=0\n",
        "while i<len(document_corpus):\n",
        " print(word_tokenize(document_corpus[i]))\n",
        " i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZUb83t4f0SH",
        "outputId": "c74549c5-d52f-4b56-96e6-ef76b5297f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['this', 'is', 'good', 'phone', 'phone']\n",
            "['this', 'is', 'bad', 'mobile', 'mobile']\n",
            "['she', 'is', 'good', 'good', 'cat']\n",
            "['he', 'has', 'bad', 'temper', 'temper']\n",
            "['this', 'mobile', 'phone', 'phone', 'is', 'not', 'good', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pop=[]\n",
        "i=0\n",
        "while i<len(document_corpus):\n",
        " pop.append(word_tokenize(document_corpus[i]))\n",
        " i+=1\n",
        "print('Token Population')\n",
        "print(pop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6RXbA-En5or",
        "outputId": "83bac85d-f159-4a64-cef4-e51057af9284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token Population\n",
            "[['this', 'is', 'good', 'phone', 'phone'], ['this', 'is', 'bad', 'mobile', 'mobile'], ['she', 'is', 'good', 'good', 'cat'], ['he', 'has', 'bad', 'temper', 'temper'], ['this', 'mobile', 'phone', 'phone', 'is', 'not', 'good', 'good']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops = set(stopwords.words('english'))\n",
        "print(stops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fybB7re5oPfN",
        "outputId": "6c9463ff-3d6a-4067-efd2-f6529edba9b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "{'once', 're', 'weren', 've', 'hasn', 'for', 'whom', 'are', 'until', \"hasn't\", 'such', 'over', 'ma', 'was', 'each', 't', 'down', 'after', 'who', 'being', 'mustn', 'into', 'did', 'he', 'has', 'what', 'while', \"wouldn't\", 'our', \"couldn't\", 'them', 'where', 'too', 'ain', 'doesn', 'been', 'yours', 'your', 'does', \"you're\", 'am', 'themselves', 'further', 'hadn', 'ourselves', \"mustn't\", 'below', \"that'll\", 'or', 'couldn', 'some', 'at', \"you'd\", 'those', 'she', 'the', 'will', \"weren't\", 'than', 'be', 'only', 'both', 'out', 'it', 'all', 'just', \"won't\", 'can', 'having', 'd', 'i', 'any', 'shan', 'herself', 'now', \"mightn't\", 'that', 'have', 'not', \"hadn't\", 'doing', 'as', 'its', 'their', \"you've\", 'him', 'during', 'me', 'we', 'to', 'in', 'under', \"you'll\", 'nor', \"didn't\", 'why', 'll', 'which', 'yourself', 'how', 'mightn', \"it's\", 'same', \"isn't\", 'myself', 'no', 'yourselves', \"aren't\", 'wouldn', 'with', 'you', 'isn', 'up', \"doesn't\", 'aren', 'if', 'between', 'and', 'shouldn', 'other', 's', 'here', 'because', 'before', 'm', 'an', 'then', 'were', 'through', 'should', 'of', 'about', 'own', 'wasn', 'they', 'against', \"she's\", 'by', 'haven', 'theirs', 'off', \"shan't\", 'my', 'a', 'don', 'himself', 'had', 'on', \"should've\", \"haven't\", 'there', 'very', 'her', 'when', 'hers', \"needn't\", 'won', 'y', 'again', 'few', 'didn', 'more', \"wasn't\", 'itself', 'needn', 'but', 'o', 'is', \"shouldn't\", 'so', \"don't\", 'from', 'above', 'these', 'do', 'this', 'his', 'ours', 'most'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "i=0\n",
        "bow=[]\n",
        "\n",
        "#Add elements\n",
        "while i<len(pop):\n",
        "  for word in pop[i]: \n",
        "    if word not in stops:\n",
        "      bow.append(word)\n",
        "  i+=1\n",
        "print(bow)\n",
        "\n",
        "#Remove commas,full stops...\n",
        "for x in bow:\n",
        "  if not x.isalpha():\n",
        "    bow.remove(x)\n",
        "\n",
        "#unique the list\n",
        "t=set(bow)\n",
        "bow=list(t)\n",
        "\n",
        "print(\"Final Processed Bag of words:\")\n",
        "print(bow)\n",
        "\n",
        "# print('Length:',len(bow))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxZOZllWodd1",
        "outputId": "03c17d39-7345-4570-de11-12de5e92a5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'phone', 'phone', 'bad', 'mobile', 'mobile', 'good', 'good', 'cat', 'bad', 'temper', 'temper', 'mobile', 'phone', 'phone', 'good', 'good']\n",
            "Final Processed Bag of words:\n",
            "['phone', 'bad', 'cat', 'good', 'mobile', 'temper']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UIqAH_Y5opKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "Vec = vectorizer.fit_transform(document_corpus)\n",
        "\n",
        "print(Vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwDQlzrwljtO",
        "outputId": "7675ec02-4777-4364-bfa0-89f5866331bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 11)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 8)\t2\n",
            "  (1, 11)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 6)\t2\n",
            "  (2, 5)\t1\n",
            "  (2, 2)\t2\n",
            "  (2, 9)\t1\n",
            "  (2, 1)\t1\n",
            "  (3, 0)\t1\n",
            "  (3, 4)\t1\n",
            "  (3, 3)\t1\n",
            "  (3, 10)\t2\n",
            "  (4, 11)\t1\n",
            "  (4, 5)\t1\n",
            "  (4, 2)\t2\n",
            "  (4, 8)\t2\n",
            "  (4, 6)\t1\n",
            "  (4, 7)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra code testing\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "token_docs = [doc.split() for doc in document_corpus]\n",
        "all_tokens = set([word for sentence in token_docs for word in sentence])\n",
        "word_to_idx = {token:idx+1 for idx, token in enumerate(all_tokens)}\n",
        "\n",
        "X = np.array([[word_to_idx[token] for token in token_doc] for token_doc in token_docs], dtype=object)\n",
        "\n",
        "X_padded = pad_sequences(X, padding=\"post\")\n",
        "\n",
        "X_df = pd.DataFrame(X_padded) \n",
        "\n",
        "print(X_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR7JyOrPm2I2",
        "outputId": "89c981e0-a992-448f-9264-f40dbab60dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0   1  2   3   4   5  6  7\n",
            "0  9  12  7   1   1   0  0  0\n",
            "1  9  12  2   8   8   0  0  0\n",
            "2  3  12  7   7   4   0  0  0\n",
            "3  5   6  2  11  11   0  0  0\n",
            "4  9   8  1   1  12  10  7  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_corpus = set()\n",
        "for row in document_corpus:\n",
        "    for word in row.split(\" \"):\n",
        "        if word not in data_corpus:\n",
        "            data_corpus.add(word)\n",
        "\n",
        "data_corpus=sorted(data_corpus)\n",
        "\n",
        "print(data_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEoRm2icnvA1",
        "outputId": "e6e87202-a851-402a-c994-516dbc9fda42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bad', 'cat', 'good', 'has', 'he', 'is', 'mobile', 'not', 'phone', 'she', 'temper', 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M0_CtAGEufva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = len(max(document_corpus, key = len).split(\" \"))\n",
        "print(length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6MdoVhJugCA",
        "outputId": "4aa5776e-804f-44e8-a998-5730171adcdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_based_encoding=[]\n",
        "for row in document_corpus:\n",
        "    row_encoding = []\n",
        "    split = row.split(\" \")\n",
        "    for i in range(length):\n",
        "        if i <= len(split)-1:\n",
        "            row_encoding.append(data_corpus.index(split[i])+1)\n",
        "        else:\n",
        "            row_encoding.append(0)\n",
        "    index_based_encoding.append(row_encoding)\n",
        "\n",
        "print(index_based_encoding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6rKt5CzvmdH",
        "outputId": "ace6a9b2-3016-436b-f183-7f7e30bb5a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[12, 6, 3, 9, 9, 0, 0, 0], [12, 6, 1, 7, 7, 0, 0, 0], [10, 6, 3, 3, 2, 0, 0, 0], [5, 4, 1, 11, 11, 0, 0, 0], [12, 7, 9, 9, 6, 8, 3, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3dBYXyIpvtKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr1 = []\n",
        "for row in document_corpus:\n",
        "    row_encoding = []\n",
        "    split = row.split(\" \")\n",
        "    for word in data_corpus:\n",
        "        if word in split:\n",
        "            row_encoding.append(1)\n",
        "        else:\n",
        "            row_encoding.append(0)\n",
        "    arr1.append(row_encoding)\n",
        "\n",
        "print(arr1)\n",
        "\n",
        "\n",
        "arr2 = []\n",
        "for row in document_corpus:\n",
        "    row_encoding = []\n",
        "    split = row.split(\" \")\n",
        "    for word in data_corpus:\n",
        "        count = split.count(word)\n",
        "        if word in split:\n",
        "            row_encoding.append(count)\n",
        "        else:\n",
        "            row_encoding.append(count)\n",
        "    arr2.append(row_encoding)\n",
        "\n",
        "print(arr2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gSHQFHKvtV6",
        "outputId": "6b685608-2fb6-4c1d-d8cd-f83965319ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1], [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1], [0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]]\n",
            "[[0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1], [1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1], [0, 1, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0], [1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0], [0, 0, 2, 0, 0, 1, 1, 1, 2, 0, 0, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GFS_VxTezlpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(document_corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDicb5NRyfOd",
        "outputId": "59406ac3-d1e8-4b10-f71f-826263922933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bad', 'cat', 'good', 'has', 'he', 'is', 'mobile', 'not', 'phone', 'she', 'temper', 'this']\n",
            "[[0 0 1 0 0 1 0 0 2 0 0 1]\n",
            " [1 0 0 0 0 1 2 0 0 0 0 1]\n",
            " [0 1 2 0 0 1 0 0 0 1 0 0]\n",
            " [1 0 0 1 1 0 0 0 0 0 2 0]\n",
            " [0 0 2 0 0 1 1 1 2 0 0 1]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(document_corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbrd1MEFzrSH",
        "outputId": "e08c5277-cdd1-43d8-a125-7e21911c00bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bad', 'cat', 'good', 'has', 'he', 'is', 'mobile', 'not', 'phone', 'she', 'temper', 'this']\n",
            "[[0.         0.         0.34273991 0.         0.         0.28832362\n",
            "  0.         0.         0.82578944 0.         0.         0.34273991]\n",
            " [0.4023674  0.         0.         0.         0.         0.28097242\n",
            "  0.80473481 0.         0.         0.         0.         0.33400129]\n",
            " [0.         0.49317635 0.6605719  0.         0.         0.27784695\n",
            "  0.         0.         0.         0.49317635 0.         0.        ]\n",
            " [0.31283963 0.         0.         0.38775666 0.38775666 0.\n",
            "  0.         0.         0.         0.         0.77551332 0.        ]\n",
            " [0.         0.         0.51309679 0.         0.         0.2158166\n",
            "  0.30906082 0.38307292 0.61812163 0.         0.         0.2565484 ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    }
  ]
}